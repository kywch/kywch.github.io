import{o as a,b as i,w as l,g as e,ad as t,v as u,x as c,T as o}from"./modules/vue-CARt6kwn.js";import{I as p}from"./slidev/default-CiUut4bT.js";import{u as m,f as d}from"./slidev/context-D_jssaYU.js";import"./index-D5A9U4U2.js";import"./modules/shiki-DvHBb6aD.js";const T={__name:"slides.md__slidev_17",setup(f){const{$clicksContext:r,$frontmatter:s}=m();return r.setup(),(g,n)=>(a(),i(p,u(c(o(d)(o(s),16))),{default:l(()=>[...n[0]||(n[0]=[e("h1",null,"Themes & Takeaways",-1),e("ul",null,[e("li",null,[e("strong",null,"Environment preparation is the new data curation"),t(" — 10K–200K verifiable environments is the critical investment, not model architecture")]),e("li",null,[e("strong",null,"Async decoupled RL infrastructure"),t(" is table stakes — all three decouple generation from training on separate GPU pools")]),e("li",null,[e("strong",null,"Agentic training"),t(" is diverging: K2.5 trains a multi-agent orchestrator (PARL), GLM-5 sequences specialized RL stages, MiniMax uses process rewards for long rollouts")]),e("li",null,[e("strong",null,"Cost is collapsing:"),t(" MiniMax at $1/hr (1/10th–1/20th the cost of Opus), with comparable performance")]),e("li",null,[e("strong",null,"The frontier is agentic, not conversational"),t(" — all three optimize for autonomous multi-step execution")])],-1)])]),_:1},16))}};export{T as default};
