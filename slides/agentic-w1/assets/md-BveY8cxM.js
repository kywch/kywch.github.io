import{o as s,b as o,w as i,g as l,ad as t,v as u,x as c,T as e}from"./modules/vue-CARt6kwn.js";import{I as p}from"./slidev/default-CiUut4bT.js";import{u as d,f as m}from"./slidev/context-D_jssaYU.js";import"./index-D5A9U4U2.js";import"./modules/shiki-DvHBb6aD.js";const x={__name:"slides.md__slidev_9",setup(g){const{$clicksContext:r,$frontmatter:a}=d();return r.setup(),(f,n)=>(s(),o(p,u(c(e(m)(e(a),8))),{default:i(()=>[...n[0]||(n[0]=[l("h1",null,"K2.5 — Agent Swarm and PARL Training",-1),l("p",null,[t("Trainable orchestrator + "),l("strong",null,"frozen"),t(" sub-agents (from intermediate checkpoints)")],-1),l("ul",null,[l("li",null,[t("Orchestrator tools: "),l("code",null,"create_subagent"),t(" + "),l("code",null,"assign_task"),t(" — sub-agents dynamically instantiated")]),l("li",null,"Training cost = wall-clock time (slowest sub-agent per stage), not total steps"),l("li",null,[t("Sub-agent trajectories "),l("strong",null,"excluded from loss"),t(" — only orchestrator is optimized")])],-1),l("p",null,[l("strong",null,"PARL reward:"),t(),l("code",null,"r = λ₁·r_parallel + λ₂·r_finish + r_perf")],-1),l("ul",null,[l("li",null,[l("code",null,"r_parallel"),t(" — rewards "),l("strong",null,"instantiation"),t(" → prevents running one agent at a time")]),l("li",null,[l("code",null,"r_finish"),t(" — rewards sub-agent "),l("strong",null,"completion"),t(" → prevents spawning agents that never finish")]),l("li",null,[t("λ₁, λ₂ "),l("strong",null,"annealed to zero"),t(" → final policy optimizes only for task performance")])],-1),l("p",null,[l("strong",null,"Observations:")],-1),l("ul",null,[l("li",null,"Wall-clock cost incentivizes balanced task decomposition, not just maximum parallelism"),l("li",null,"Orchestrator trained first with small (cheap) frozen sub-agents for faster rollouts, then scaled up to larger sub-agents — coordination patterns transfer")],-1),l("p",null,[l("strong",null,"Result:"),t(" 3–4.5× latency reduction; item-F1 72.8% → 79.0% on WideSearch")],-1)])]),_:1},16))}};export{x as default};
